{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonconjugate model for a bioassay experiment (BDA3 p.74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit  \n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../data/bioassay.csv')\n",
    "x = df.dose.values\n",
    "n = df.num_animals.values\n",
    "y = df.num_deaths.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood estimate of ($\\alpha, \\beta$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       point_estimate  standard_error  Wald_statistics\n",
      "alpha        0.846554        1.019077         0.815156\n",
      "beta         7.748719        4.872704         0.326355\n"
     ]
    }
   ],
   "source": [
    "# structure data in individual observation format for fitting logistic regression\n",
    "# it is feasible to use glm in R for fitting a count table. However, it's not possible for LogisticRegression in sklearn\n",
    "xx = np.repeat(x,n).reshape(-1,1)\n",
    "yy = [[1]*y[i] + [0]*(n[i]-y[i]) for i in range(len(y))]\n",
    "yy = np.asarray(yy).ravel()\n",
    "lm = LogisticRegression(penalty='none', solver='lbfgs').fit(xx,yy)\n",
    "\n",
    "# M.L.E point estimate\n",
    "alpha_hat, beta_hat = np.asscalar(lm.intercept_), np.asscalar(lm.coef_)\n",
    "\n",
    "# Design matrix -- add column of 1's at the beginning of X matrix\n",
    "X_design = np.hstack([np.ones((xx.shape[0], 1)), xx])\n",
    "# Initiate matrix of 0's, fill diagonal with each predicted observation's variance\n",
    "p_hat = lm.predict_proba(xx)\n",
    "V = np.diagflat(np.product(p_hat, axis=1))\n",
    "\n",
    "# Covariance matrix\n",
    "# Note that the @-operater does matrix multiplication in Python 3.5+, so if you're running\n",
    "# Python 3.5+, you can replace the covLogit-line below with the more readable:\n",
    "# cov_logit = np.linalg.inv(X_design.T @ V @ X_design)\n",
    "cov_logit = np.linalg.inv(np.dot(np.dot(X_design.T, V), X_design))\n",
    "# print(\"Covariance matrix: \", cov_logit)\n",
    "\n",
    "# Standard errors\n",
    "alpha_se, beta_se = np.sqrt(np.diag(cov_logit))\n",
    "# Wald statistic (coefficient / s.e.) ^ 2\n",
    "alpha_w, beta_w = alpha_hat / alpha_se**2, beta_hat / beta_se**2\n",
    "\n",
    "mle_df = pd.DataFrame({'point_estimate':[alpha_hat, beta_hat]\n",
    "                       , 'standard_error':[alpha_se, beta_se]\n",
    "                       , 'Wald_statistics': [alpha_w, beta_w]}\n",
    "                     , index=['alpha','beta'])\n",
    "print(mle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the posterior density in grid\n",
    "A = np.linspace(-4, 8, 100)\n",
    "B = np.linspace(-10, 40, 100)\n",
    "ilogit_abx = 1 / (np.exp(-(A[:,None] + B[:,None,None] * x)) + 1)\n",
    "p = np.prod(ilogit_abx**y * (1 - ilogit_abx)**(n - y), axis=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
